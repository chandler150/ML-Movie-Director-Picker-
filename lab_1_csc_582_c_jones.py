# -*- coding: utf-8 -*-
"""Lab_1-CSC_582-C_Jones.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13nBqYNPXejN1qD3PGwTpfzX-XIjP0Ccm

The situation is this: People have lots of ideas about movies. They can write a short synopsis of the plot. What they want to know is who should be acting in the movie? Who should direct it? And lastly, what should the title be?

So, imagine you get an email (example below) that has a description of a movie. Your job is to reply back with suggestions of 5-10 actors, a director and title. That's it! 

The main deliverable is a python3 program: robotproducer.py [input.txt]

Where input.txt is a plaintext file with content similar to the following:

-----------------

 

This is a no holds-barred thrilling drama mixed with killing, mayhem and manipulation among working professionals. This film sheds light on a man's downfall from the pinnacles of success into the depths of his damaged character. His insecurities lead him into a series of troubled romantic relationships and eventually a web of events that include betrayal and murder.

-----------------

Output to the screen should be like the following;

Title suggestion: A crazy work week!

Director suggestion: SOME NAME

Cast suggestions: NAME1, NAME2, NAME3, ...

 

I was just kidding!! This is actually a very difficult, cutting-edge problem. So, know that I'm not interested in perfection just interesting approaches. 

 

Use the Kaggle database TMDB Movie metadata: https://www.kaggle.com/tmdb/tmdb-movie-metadata (Links to an external site.)

It has two data files totaling about 50MB. We are interested in three main columns (but you may use any column available, they would actually help). The three columns are: Title, Overview, Cast (only names) and Crew:Director. 

 

How to evaluate this?

The title is simply very difficult to evaluate, so that's just something I can assess based on creativity. 

However, the director and the cast can be numerically evaluated if you set up the problem as a machine learning exercise. Divide all the movies into training/test randomly with appropriate ratios. Then use the description (overview) of one of the test set movies as input into your program. Evaluate the output like this:

+ 20 points if you get the director right. (20 max)

+ 10 points for every cast member that you guessed that appears on the cast list. Up to 50 maximum points.

+ 5 points for every cast member guessed correctly that also appears on the top 5 (orders 0-4) of the cast list. (25 max)

 

Now, optimize for maximum possible score (95).

Please include 2-3 page report.pdf file where you explain your approach on title, director, cast and print out a few examples and evaluation results on your own test set.

Submit your code and readme via Polylearn as a zip file. If you did this on Google Collab, you may download as Python then submit that file. In addition, to submitting the downloaded .py (from Collab), also share your collab notebook with me (foaadk@gmail.com) and submit a link via Polylearn online text.
"""

# Commented out IPython magic to ensure Python compatibility.
import sys
import pandas as pd
import numpy as np
import json
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
import re
import csv
import matplotlib.pyplot as plt 
import seaborn as sns
from tqdm import tqdm
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
def alltheotherstuff(short_text):
    '''This function takes the given movie dataset, adds it to a pandas dataframe,
     cleans the data, appends requisite columns (Director, Top 5 Actors), and trains
      a machine learning model to predict the genre using the de-stopworded overview.

      This should be shortened to multiple smaller functions ASAP.'''
    # %matplotlib inline
    pd.set_option('display.max_colwidth', 300)

    #Step one, Get the data
    import io
    df = pd.read_csv("tmdb_5000_movies.csv")
    cacr = pd.read_csv("tmdb_5000_credits.csv")
    # Dataset is now stored in a Pandas Dataframe
    #print(df, cacr)

    #Step two, Get collumn names

    #for col in df.columns:
    #    print(col)

    #for col in cacr.columns:
    #    print('hi',col)

    # Send data from credits to mvoies
    df['cast'] = cacr['cast']
    df['crew'] = cacr['crew']

    #Step Three, Delete uneccesary columns
    del df['original_language']
    del df['homepage']
    del df['popularity']
    del df['production_companies']
    del df['production_countries']
    del df['release_date']
    del df['revenue']
    del df['runtime']
    del df['spoken_languages']
    del df['status']
    del df['original_title']
    del df['vote_average']
    del df['vote_count']
    del df['budget']

    #print(df)
    #for col in df.columns:
    #    print(col)


    #Clean Columns

    #Clean genres
    newg = []
    newf = []
    #df['genres'][0]
    for i in range(len(df['genres'])):
      for l in range(len(json.loads(df['genres'][i]))):
        newg.append(list(json.loads(df['genres'][i])[l].values())[1].lower())
      newf.append(newg)
      newg = []
    df['new_genre'] = newf

    #Clean keywords
    newg = []
    newf = []
    for i in range(len(df['keywords'])):
      for l in range(len(json.loads(df['keywords'][i]))):
        newg.append(list(json.loads(df['keywords'][i])[l].values())[1].lower())
      newf.append(newg)
      newg = []
    df['new_keywords'] = newf

    # function for text cleaning
    def clean_text(text):
        # remove backslash-apostrophe
        text = re.sub("\'", "", str(text))
        # remove everything except alphabets
        text = re.sub("[^a-zA-Z]"," ",str(text))
        # remove whitespaces
        text = ' '.join(text.split())
        # convert text to lowercase
        text = text.lower()

        return text

    df['clean_overview'] = df['overview'].apply(lambda x: clean_text(x))

    stop_words = set(stopwords.words('english'))

    # function to remove stopwords
    def remove_stopwords(text):
        no_stopword_text = [w for w in text.split() if not w in stop_words]
        return ' '.join(no_stopword_text)

    df['clean_overview'] = df['clean_overview'].apply(lambda x: remove_stopwords(x))

    #Get Directors
                                                    #print(df['crew'])
                                                    #print(list(df['crew'][1]))
    ds = []
    dn = []
    for l in range(len(df['crew'])):
      for i in range(len(json.loads(df['crew'][l]))):
                                                    #print(list(json.loads(df['crew'][l])[i].values()))
        hi = (list(json.loads(df['crew'][l])[i].values()))
        if hi[1] == 'Directing':
          if hi[4] == "Director":
            ds.append(hi[5])
      dn.append(ds)
      ds = []
    df['Directors'] = (dn)
    #print(df['Directors'])

                                                    #print(len(ds))

    # Get Main Actors

    #print(df)
    #print(df['cast'])
    actors = []
    nactors = []
    for i in range(len(df['cast'])):
      for l in range(0,5):
        try:
          nactors.append(json.loads(df['cast'][i])[l]['name'])
        except:
          nactors.append('No_more_Characters')
      actors.append(nactors)
      nactors = []

    df["Actors"] = actors

    # Code for getting Director names:
    #for col in df.columns:
        #print(col)
    #df['Actors']

    #working dataframe

    def freq_words(x, terms = 30):
      all_words = ' '.join([text for text in x])
      all_words = all_words.split()
      fdist = nltk.FreqDist(all_words)
      words_df = pd.DataFrame({'word':list(fdist.keys()), 'count':list(fdist.values())})

      # selecting top 20 most frequent words
      d = words_df.nlargest(columns="count", n = terms)

      # visualize words and frequencies
      plt.figure(figsize=(12,15))
      ax = sns.barplot(data=d, x= "count", y = "word")
      ax.set(ylabel = 'Word')
      plt.show()

    # print 100 most frequent words
    #freq_words(df['clean_overview'], 100)
    #df

    del df['genres']
    del df['keywords']
    del df['overview']
    del df['cast']
    del df['crew']
    #df

    # Now that dataset is cleaned, delete any row with absent data
    df1 = df
    df.new_genre.str.len().eq(0)
    df2 = df1[df1['new_keywords'].map(lambda d: len(d)) > 0]
    df3 = df2[df2['new_genre'].map(lambda d: len(d)) > 0]

    #df3
    #df3

    # Now with fully cleaned data, lets split a training set
    df = df3

    # Something something one hot
    from sklearn.preprocessing import MultiLabelBinarizer

    multilabel_binarizer = MultiLabelBinarizer()
    multilabel_binarizer.fit(df['new_genre'])

    # transform target variable
    y = multilabel_binarizer.transform(df['new_genre'])
    tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=10000)

    # split dataset into training and validation set
    xtrain, xval, ytrain, yval = train_test_split(df['clean_overview'], y, test_size=0.2, random_state=9)

    # create TF-IDF features
    xtrain_tfidf = tfidf_vectorizer.fit_transform(xtrain)
    xval_tfidf = tfidf_vectorizer.transform(xval)

    # now lets train a Genre guesser
    from sklearn.linear_model import LogisticRegression

    # Binary Relevance
    from sklearn.multiclass import OneVsRestClassifier

    # Performance metric
    from sklearn.metrics import f1_score

    lr = LogisticRegression()
    clf = OneVsRestClassifier(lr)

    # fit model on train data
    clf.fit(xtrain_tfidf, ytrain)

    # make predictions for validation set
    y_pred = clf.predict(xval_tfidf)

    y_pred[3]

    multilabel_binarizer.inverse_transform(y_pred)[3]

    # evaluate performance
    f1_score(yval, y_pred, average="micro")

    # predict probabilities
    y_pred_prob = clf.predict_proba(xval_tfidf)

    t = .00001 # threshold value
    y_pred_new = (y_pred_prob >= t).astype(int)

    # evaluate performance
    f1_score(yval, y_pred_new, average="micro")

    def infer_tags(q):
        q = clean_text(q)
        q = remove_stopwords(q)
        q_vec = tfidf_vectorizer.transform([q])
        q_pred = clf.predict(q_vec)
        return multilabel_binarizer.inverse_transform(q_pred)

    for i in range(5):
      k = xval.sample(1).index[0]
      #print("Movie: ", df['title'][k], "\nPredicted genre: ", infer_tags(xval[k])), print("Actual genre: ",df['new_genre'][k], "\n")

    breakpoint()
    return [df, infer_tags(short_text)]

def cleantext(text):
    # remove backslash-apostrophe
    text = re.sub("\'", "", str(text))
    # remove everything except alphabets
    text = re.sub("[^a-zA-Z]", " ", str(text))
    # remove whitespaces
    text = ' '.join(text.split())
    # convert text to lowercase
    text = text.lower()
    return text

def remove_stop(text):
    stop_words = set(stopwords.words('english'))
    no_stopword_text = [w for w in text.split() if not w in stop_words]
    return ' '.join(no_stopword_text)

def Fileimport():
    """This function imports a plaintext .txt file from the command line.
        It runs in main(), and places the txt file into a string, 'data'
        Make sure to specify the whole path to your file"""
    file_name = sys.argv[1]
    f = open(file_name)
    data = f.read()
    return data
    f.close()
def backofthehandcalcs(given):
    '''This function takes the given movie overview and uses our model to
     generate a plausible genre. This is moderately reliable'''
    clean = cleantext(given)
    short = remove_stop(clean)
    return short
def give_output():
    return

## Given that infer_tags(xval[k]) invokes the overview of a movie, let us then develop main script

def main():
    given = Fileimport()
    short_text = backofthehandcalcs(given)
    data = alltheotherstuff(short_text)
    breakpoint()
    output = give_output()

    print("output is:")
if __name__ == "__main__":
    main()
